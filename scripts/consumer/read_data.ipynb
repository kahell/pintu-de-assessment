{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"reader\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema of the CSV file, if known.\n",
    "# If the CSV file has a header, you can let Spark infer the schema automatically.\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"order_side\", StringType(), True),\n",
    "    StructField(\"size\", DoubleType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"created_at\", IntegerType(), True),\n",
    "    StructField(\"total\", DoubleType(), True),\n",
    "    StructField(\"cum_sum\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your local CSV file\n",
    "this_script_dir = os.getcwd()\n",
    "output_dir_path_string = os.path.join(\n",
    "    os.path.dirname(\n",
    "        this_script_dir\n",
    "    ),\n",
    "    '..',\n",
    "    'src',\n",
    "    'main',\n",
    "    'scala',\n",
    "    'org',\n",
    "    'pintu',\n",
    "    'output',\n",
    "    'date_partition=2023-11-06'\n",
    ")\n",
    "\n",
    "source_dir_path_string = os.path.join(\n",
    "    os.path.dirname(\n",
    "        this_script_dir\n",
    "    ),\n",
    "    'producer',\n",
    "    'order_book_mockup.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet data\n",
    "df = spark.read.parquet(output_dir_path_string)\n",
    "\n",
    "# Read Source data\n",
    "source_df = spark.read.csv(path=source_dir_path_string, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame content\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()\n",
    "# Before: 7825\n",
    "# After Filtered: 5985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.count()\n",
    "# Before: 7826\n",
    "# After: 7826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_status_df = df.filter(col('status') == \"CLOSED\")\n",
    "filtered_status_df.count()\n",
    "# Before: 30\n",
    "# After: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_order_side_buy_df = df.filter(col('side') == \"BUY\")\n",
    "filtered_order_side_buy_df.show()\n",
    "filtered_order_side_buy_df.count()\n",
    "# SELL: 2990\n",
    "# BUYL: 2995\n",
    "# Total: 5985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_order_side_sell_df = df.filter(col('side') == \"SELL\")\n",
    "filtered_order_side_sell_df.show()\n",
    "filtered_order_side_sell_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = source_df.filter(col('order_id') == 110)\n",
    "filtered_df.show()\n",
    "\n",
    "# Wed, 06 Sep 2023 07:08:28 GMT\n",
    "# Wednesday, September 6, 2023 6:38:28 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_filtered_status_df = source_df.filter(col('status') == \"CLOSED\")\n",
    "source_filtered_status_df.count()\n",
    "# Before: 1826\n",
    "# After: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_filtered_order_side_df = source_df.filter(col('order_side').isNull())\n",
    "source_filtered_order_side_df.count()\n",
    "# SELL: 2994\n",
    "# BUY: 3006\n",
    "# NULL: 1826\n",
    "# Total: 7826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window specification\n",
    "windowSpec = Window.partitionBy('order_id').orderBy('created_at').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "# Add a new column 'isClosed' which will be true if the last status in the window is 'CLOSED'\n",
    "source_dedup_df = source_df.withColumn('isClosed', when(last(col('status')).over(windowSpec) == 'CLOSED', True).otherwise(False))\n",
    "source_dedup_df.show()\n",
    "\n",
    "# OPEN:   Wednesday, September 6, 2023 4:05:08 AM\n",
    "# CLOSED: Wednesday, September 6, 2023 5:50:08 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure to use parentheses properly and '==' for equality checks.\n",
    "# Also, use the bitwise '&' for 'and' operations within the filter function.\n",
    "source_open_dedup_df = source_dedup_df.filter((col(\"isClosed\") == False) & (col(\"status\") == \"OPEN\"))\n",
    "\n",
    "# Show the results\n",
    "source_open_dedup_df.show()\n",
    "\n",
    "# Count the number of rows\n",
    "source_open_dedup_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_open_dedup_df = source_open_dedup_df.withColumn(\"total\", col(\"size\") * col(\"price\"))\n",
    "\n",
    "# Define the window specification\n",
    "windowSpec = Window.partitionBy('order_side').orderBy('created_at').rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Calculate the cumulative sum within each partition defined by 'order_side'\n",
    "source_cumsum_df = source_open_dedup_df.withColumn('cumulative_sum', sum(col(\"total\")).over(windowSpec))\n",
    "source_cumsum_df.show()\n",
    "\n",
    "# OPEN:   Wednesday, September 6, 2023 4:05:08 AM\n",
    "# CLOSED: Wednesday, September 6, 2023 5:50:08 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window specification for row number without ordering as ordering is not required for this operation\n",
    "window_spec = Window.partitionBy(\"order_side\").orderBy(lit(0))\n",
    "\n",
    "# Filter BUY and SELL into separate DataFrames and add row numbers\n",
    "buy_df = source_cumsum_df.filter(col(\"order_side\") == \"BUY\") \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .withColumnRenamed(\"order_side\", \"buy_side\") \\\n",
    "    .withColumn(\"buy_side\", concat(lit(\"buy_\"), col(\"row_num\")))\n",
    "\n",
    "sell_df = source_cumsum_df.filter(col(\"order_side\") == \"SELL\") \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .withColumnRenamed(\"order_side\", \"sell_side\") \\\n",
    "    .withColumn(\"sell_side\", concat(lit(\"sell_\"), col(\"row_num\")))\n",
    "\n",
    "# Combine BUY and SELL with a union and sort by row number\n",
    "combined_df = buy_df.select(col(\"buy_side\").alias(\"side\"), \"*\") \\\n",
    "    .union(sell_df.select(col(\"sell_side\").alias(\"side\"), \"*\")) \\\n",
    "    .sort(\"row_num\")\n",
    "\n",
    "combined_df.show()\n",
    "combined_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pintu-sr-de-assesment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
